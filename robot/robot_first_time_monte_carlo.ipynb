{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from robot import RecycleEnv\n",
    "class MonteCarloAgent:\n",
    "    def __init__(self,env,gamma=0.9,theta=0.01):\n",
    "        self.name = \"Monte Carlo Agent\"\n",
    "        self.env = env\n",
    "        self.V = {\"low\": 0, \"high\": 0}\n",
    "        self.pi = {\"low\": self.env.getPossibleActions(\"low\"), \"high\": self.env.getPossibleActions(\"high\")}\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "\n",
    "    \n",
    "    def create_episodes(self, num_episodes, episode_length):\n",
    "        episodes = []\n",
    "    \n",
    "        for _ in range(num_episodes):\n",
    "            episode = []\n",
    "            self.env.reset()\n",
    "        \n",
    "            for _ in range(episode_length):\n",
    "                current_state = self.env.state\n",
    "                action = random.choice(self.pi[current_state])\n",
    "                next_state, reward, _, _, _ = self.env.step(action)\n",
    "                episode.append((current_state, action, reward))\n",
    "            \n",
    "            episodes.append(episode)\n",
    "        \n",
    "        return episodes\n",
    "\n",
    "    def calculate_values(self, episodes):\n",
    "        returns = {\"low\": [], \"high\": []}\n",
    "    \n",
    "        for episode in episodes:\n",
    "            G = 0\n",
    "            # Process each state in the episode in reverse order\n",
    "            for t in range(len(episode)-1, -1, -1):\n",
    "                state, _, reward = episode[t]\n",
    "                G = self.gamma * G + reward\n",
    "            \n",
    "                # Check if this is the first occurrence of the state in the episode\n",
    "                first_occurrence = True\n",
    "                for i in range(t):\n",
    "                    if episode[i][0] == state:\n",
    "                        first_occurrence = False\n",
    "                        break\n",
    "                    \n",
    "                if first_occurrence:\n",
    "                    returns[state].append(G)\n",
    "    \n",
    "                \n",
    "        # Calculate average returns for each state\n",
    "            for state in returns:\n",
    "                if returns[state]:  # Check if we have any returns for this state\n",
    "                    self.V[state] = sum(returns[state])/len(returns[state])\n",
    "\n",
    "    def estimate_value_monte_carlo(self, num_episodes=10, episode_length=100):\n",
    "        episodes = self.create_episodes(num_episodes, episode_length)\n",
    "        self.calculate_values(episodes)\n",
    "\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "        states = [\"low\", \"high\"]\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for state in states:\n",
    "                v = self.V[state]\n",
    "                value = self.calculate_state_value(state)\n",
    "                self.V[state] = value\n",
    "                delta = max(delta, abs(v - self.V[state]))\n",
    "\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "    \n",
    "    def calculate_state_value(self,state):\n",
    "        actions = self.pi[state]\n",
    "        value = 0\n",
    "        for action in actions:\n",
    "            value += (1/len(actions)) *self.calculate_q_value(state,action)\n",
    "        return value\n",
    "\n",
    "\n",
    "    def calculate_q_value(self,state,action):\n",
    "        sum = 0\n",
    "        for transition in self.env.getTransitionStatesandProbs(state, action):\n",
    "            next_state, reward, prob, _ = transition\n",
    "            sum += prob * (reward + self.gamma * self.V[next_state])\n",
    "        return sum\n",
    "    def reset_values(self):\n",
    "        self.V = {\"low\": 0, \"high\": 0}\n",
    "\n",
    "env = RecycleEnv()\n",
    "agent = MonteCarloAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'low': 14.858268593014833, 'high': 18.06617114887673}\n"
     ]
    }
   ],
   "source": [
    "agent.estimate_value_monte_carlo(2000)\n",
    "print(agent.V)\n",
    "agent.reset_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'low': 14.788357138388012, 'high': 17.91216501012827}\n"
     ]
    }
   ],
   "source": [
    "agent.policy_evaluation()\n",
    "print(agent.V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
