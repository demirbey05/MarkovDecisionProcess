{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10aebfa6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6b12e2d85e58cb32420a0e44e2f86bc",
     "grade": false,
     "grade_id": "cell-d882ed5f5256f39b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Dynamic Programming for the Recycle Robot Environment\n",
    "\n",
    "(Implementation of Example 3.3 in Sutton & Barto textbook, 2nd edition.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da4dcd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7447e9f887f111c1bfd27900b69b6aaa",
     "grade": false,
     "grade_id": "cell-1e5fcb527759f2fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<img src=\"./images/RecyclingRobot_mod.jpg\" \n",
    "     align=\"center\" \n",
    "     width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8863c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a44e4c17ec66b44ddb3f79c0fbe164c",
     "grade": false,
     "grade_id": "cell-0d83b8843c0d5406",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9466431",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f4193a7daef2198ffeeb94218d5965b",
     "grade": false,
     "grade_id": "cell-2e17d190a97bdc17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gymnasium import Env, spaces\n",
    "from gymnasium.envs.toy_text.utils import categorical_sample\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "import time\n",
    "random.seed(5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f44bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4dda69008439bbb2f79e52a1cf32acaf",
     "grade": false,
     "grade_id": "cell-2d2026eb1e74eeaf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Create MDP for the Recycling Robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8995c32f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b23e1e57ef049244e92b5aba29eb245",
     "grade": false,
     "grade_id": "cell-67e72adfe133a7c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RecycleEnv(Env):\n",
    "\n",
    "    def __init__(self, alpha=0.5, beta=0.2, duration=20,r_search=4, r_wait=1):\n",
    "        self.observation_space = spaces.Discrete(2)  # 0: low 1: high\n",
    "        self.action_space = spaces.Discrete(3)  # 0: search 1: wait 2: recharge\n",
    "\n",
    "        self.state = \"high\"\n",
    "        self.sum = 0\n",
    "\n",
    "        self.StateActionPairings = {\"low\": [\"search\", \"wait\", \"recharge\"],\n",
    "                                    \"high\": [\"search\", \"wait\"]}\n",
    "\n",
    "        # action: {state:[(nextstate, reward, probability, termination)]}\n",
    "        self.TransitionStatesandProbs = {\n",
    "            \"wait\": {\n",
    "                \"high\": [(\"high\", r_wait, 1, False)],\n",
    "                \"low\": [(\"low\", r_wait, 1, False)]\n",
    "            },\n",
    "            \"recharge\": {\n",
    "                \"low\": [(\"high\", 0, 1, False)]\n",
    "            },\n",
    "            \"search\": {\n",
    "                \"high\": [(\"high\", r_search, round(alpha, 1), False),\n",
    "                         (\"low\", r_search, round(1-alpha, 1), False)],\n",
    "                \"low\": [(\"low\", r_search, round(beta, 1), False),\n",
    "                        (\"high\", -3, round(1-beta, 1), False)]\n",
    "            }}\n",
    "\n",
    "    def reset(self, seed=None, options: Optional[dict] = None):\n",
    "        self.state = \"high\"\n",
    "        self.time = 0\n",
    "        self.sum = 0\n",
    "        print(\"Environment reset: Current state is High\")\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, a):\n",
    "        transitions = self.TransitionStatesandProbs[a][self.state]\n",
    "        i = categorical_sample([t[2] for t in transitions], self.np_random)\n",
    "        s, r, p, t = transitions[i]\n",
    "        self.SumReward(r)\n",
    "        self.state = s\n",
    "        return (s, r, t, False, {})\n",
    "\n",
    "    def SumReward(self, reward=0):\n",
    "        self.sum = self.sum + reward\n",
    "        return self.sum\n",
    "\n",
    "    def getPossibleActions(self, state):\n",
    "        return self.StateActionPairings[state]\n",
    "\n",
    "    def getTransitionStatesandProbs(self, state, action):\n",
    "        return self.TransitionStatesandProbs[action][state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87741c-17f1-4618-86bd-e7e927b3cd00",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2e2e4be5c26efcb58caccf3945f3061",
     "grade": false,
     "grade_id": "cell-69ee89f3f5e4f1f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Agent using Value Iteration to find the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "983f7fa5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b22ed8871fb6e85f0da5f514831de1b",
     "grade": false,
     "grade_id": "cell-fbdd7fabe3dc0121",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ValueIterationAgent():\n",
    "    def __init__(self, env, discount=0.9, iterations=100, theta=0.01):\n",
    "        self.name = \"Value Iteration Agent\"\n",
    "        self.env = env\n",
    "        self.discount = discount\n",
    "        self.theta = theta\n",
    "        self.V = {\"low\": 0, \"high\": 0}\n",
    "        self.pi = {}\n",
    "\n",
    "        # train\n",
    "        self.train(iterations)\n",
    "\n",
    "    def train(self, iterations):\n",
    "        states = [\"low\", \"high\"]  # states are numbered\n",
    "        start_time = time.time()\n",
    "        for i in range(iterations):\n",
    "            delta = 0\n",
    "\n",
    "            for state in states:\n",
    "                v = self.V[state]\n",
    "                actions = self.env.getPossibleActions(state)\n",
    "                action_value_list = []\n",
    "                for action in actions:\n",
    "\n",
    "                    ###################################################\n",
    "                    # YOU WILL IMPLEMENT THIS FUNCTION IN TASK 1\n",
    "                    q_val_for_given_action = self.calculate_q_value_estimate(state, action)\n",
    "                    ###################################################\n",
    "\n",
    "                    action_value_list.append(q_val_for_given_action)\n",
    "\n",
    "                ###################################################\n",
    "                # YOU WILL IMPLEMENT THIS FUNCTION IN TASK 2\n",
    "                self.V[state] = self.update_state_value_function(action_value_list)  # max(value_list_for_actions)\n",
    "                ###################################################\n",
    "\n",
    "                ###################################################\n",
    "                # YOU WILL IMPLEMENT THIS FUNCTION IN TASK 3\n",
    "                delta = self.update_delta(delta, v, state)  # max(delta, abs(v - self.V[state]))\n",
    "                ###################################################\n",
    "\n",
    "                # explicit \"policy improvement\"\n",
    "                self.pi[state] = actions[action_value_list.index(self.V[state])]\n",
    "\n",
    "            # stopping criterion\n",
    "            if delta < self.theta:\n",
    "                print(f\"Stopping criterion satisfied after {i} iterations.\\n\")\n",
    "                print(f\"Value function is: {self.V}\\n\")\n",
    "                print(f\"Optimal policy is: {self.pi}\\n\")\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(f\"Execution time: {elapsed_time:.4f} seconds\\n\")  # Print execution time\n",
    "\n",
    "                break\n",
    "\n",
    "    def getValue(self, state):\n",
    "        return self.V[state]\n",
    "\n",
    "    def getPolicy(self, state):\n",
    "        return self.pi[state]\n",
    "\n",
    "    def getAction(self, state):\n",
    "        return self.getPolicy(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b218a02-3172-46d1-9ce3-9090b016e7b2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ec1d5614104958be36a210f6493f9ac",
     "grade": false,
     "grade_id": "cell-734a145807a9cab1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## Task 1\n",
    "\n",
    "Your task is to implement the method `calculate_q_value` that takes as input a state and an action (see `ValueIterationAgent`).\n",
    "In this method, there is already a loop implemented over all possible next states and their reward.\n",
    "In the loop, you therefore have access to the transition probability $ p(s', r \\mid s, a) $, here `trans_prob`, the next state $s'$, here `next_state`, and the reward $r$, here `reward`.\n",
    "\n",
    "Based on these variables and the attributes of `ValueIterationAgent`, calculate the action-value function estimate of the current iteration.\n",
    "\n",
    "_Hint: Note that the current state-value function estimate is stored in `self.V`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4f43cf-15fd-404b-8b02-822746f9069c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44d036f45e6366b84e0e744f8dca87b6",
     "grade": false,
     "grade_id": "cell-2ca1d2d82d3e6d64",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_q_value_estimate(self, state, action):\n",
    "\n",
    "    q = 0\n",
    "    for sum_element in self.env.getTransitionStatesandProbs(state, action):\n",
    "\n",
    "        # Extract information\n",
    "        next_state = sum_element[0]  # next state s'\n",
    "        reward = sum_element[1]  # reward r\n",
    "        trans_prob = sum_element[2]  # transition probability p(s', r | s, a)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        q += trans_prob * (reward + self.discount * self.V[next_state])\n",
    "\n",
    "    return q\n",
    "\n",
    "\n",
    "# This adds the function as a method to the specified class.\n",
    "setattr(ValueIterationAgent, \"calculate_q_value_estimate\", calculate_q_value_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9709ac-6216-4ca4-aa84-b969f5304e9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb2c96e2a3bb7c85e2b6b4b7829215e5",
     "grade": false,
     "grade_id": "cell-47ef8ce067ec5908",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## Task 2\n",
    "\n",
    "Your task is to implement the method `update_state_value_function` that takes as input a list of action-value functions estimates $q(s,a)$ for all actions given the state $s$ (see `ValueIterationAgent`).\n",
    "Based on this list, calculate the state-value function $v(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26b411df-16e8-4a5d-a2c2-29f2a8097296",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb367b4fd01cc94e5a818a419ba57d81",
     "grade": false,
     "grade_id": "cell-8e137ddfdac18198",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_state_value_function(self, list_of_q_values):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    return max(list_of_q_values)\n",
    "\n",
    "\n",
    "# This adds the function as a method to the specified class.\n",
    "setattr(ValueIterationAgent, \"update_state_value_function\", update_state_value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462cf60a-4e0f-4d6b-b5e0-dd24d70c1e71",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d113875f3757c7116cfc4c918ec5bc4",
     "grade": false,
     "grade_id": "cell-2eac678135b83911",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Task 3\n",
    "\n",
    "Your task is to implement the method `update_delta` that takes as input the current $\\Delta$, here `delta`, the state-value function before the update, here `v`, and the state, here `state`.\n",
    "Based on this list, calculate the updated $\\Delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "262d06a0-7f7f-4678-adbe-a93dd36a0ffe",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e042510f7519ce8bc6ba8643057f83dc",
     "grade": true,
     "grade_id": "cell-17225a2e1c3be77d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_delta(self, delta, v, state):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    return max(delta, abs(v - self.V[state]))\n",
    "\n",
    "\n",
    "# This adds the function as a method to the specified class.\n",
    "setattr(ValueIterationAgent, \"update_delta\", update_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d2dfc-eaed-4557-809c-c27e02d690f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efb1aa627bae9bb94c1903283eccd2d4",
     "grade": false,
     "grade_id": "cell-8af0a4da44b03a69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Dummy Agent using the random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84515cdc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "200e09636d84def3f18b0b8ea7da4ea4",
     "grade": false,
     "grade_id": "cell-8777b142e1b0cecf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DummyAgent():\n",
    "    def __init__(self, env):\n",
    "        self.name = \"Dummy Agent\"\n",
    "        self.env = env\n",
    "\n",
    "    def getValue(self, state):\n",
    "        pass\n",
    "\n",
    "    def getPolicy(self, state):\n",
    "        actions = self.env.getPossibleActions(state)\n",
    "        return random.choice(actions)\n",
    "\n",
    "    def getAction(self, state):\n",
    "        return self.getPolicy(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f278c2-7fea-4364-88f9-3f159e38a403",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "428982e5011a6ec40a3a74abee260da1",
     "grade": false,
     "grade_id": "cell-ea1a139ce1c288b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Define play-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d41be4a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25e1ff49a09ef03c8730522c7a29f8f7",
     "grade": false,
     "grade_id": "cell-b08f0b97a84c4086",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def play(agent, env, rounds=100):\n",
    "    print(\"#\"*10, \"\\n\")\n",
    "    print(f\"Starting play with agent: {agent.name}\\n\")\n",
    "\n",
    "    for round_no in range(rounds):\n",
    "        current_state = env.state\n",
    "        player_move = agent.getAction(current_state)\n",
    "        next_state, r, t, _, _ = env.step(player_move)\n",
    "\n",
    "        print(f\"Round {round_no} \\nAction: {player_move}\\tReward: {r}\\tNext State: {next_state}\")\n",
    "        img = Image.open(\"./images/\"+current_state+\"_\"+player_move+\"_\"+next_state+\".jpg\")\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        input(\"Press Enter to continue!\")\n",
    "\n",
    "    print(f\"{rounds} Rounds Ended:\\n\")\n",
    "    print(f\"The agent collected a total return of {env.sum}\\n\")\n",
    "    env.reset()\n",
    "    print(\"#\"*10, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce099df1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfd24067751c0af65f9d5df82198394c",
     "grade": false,
     "grade_id": "cell-a1934f91f68843ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Dummy Agent\n",
    "\n",
    "Test the behavior of a recycling robot with random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b22df1b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8816cfcbc583e251476c3450ec2dd623",
     "grade": false,
     "grade_id": "cell-0a49ad16cd74f318",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = RecycleEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DummyAgent(env)\n",
    "play(agent, env, rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bda03d-9440-44c9-87a1-471cd98ef58f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3d27cadc11d57afdf0948e09c1a2fae",
     "grade": false,
     "grade_id": "cell-d9d4becd5723831f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Value Iteration Agent\n",
    "\n",
    "Test the optimal behavior of the recycling robot given the specified parametrization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365dbaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ValueIterationAgent(env=env)\n",
    "play(agent, env, rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86872103-be67-4f3f-a42c-744f062dbb0f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f633639dabc4904265b30c6771d151b",
     "grade": false,
     "grade_id": "cell-d732f3619e354836",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Questions\n",
    "\n",
    "**Question 1.** Suppose the reward for waiting is higher than for searching. What do you expect will happen to the value functions of the two states? What would be the optimal policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd8d10-eb31-453b-94ef-56c48643d6c0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d02bd084ee4beedcba3cdb69d07d658f",
     "grade": true,
     "grade_id": "cell-05d8f68f1d64cf37",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bcd63-3d16-41ee-8165-c379f27e489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RecycleEnv( r_search=1, r_wait=4)\n",
    "agent = ValueIterationAgent(env=env)\n",
    "\n",
    "play(agent, env, rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a9b9f6",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c37030f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIterationAgent():\n",
    "    def __init__(self, env, discount=0.9, iterations=100, theta=0.01):\n",
    "        self.name = \"Policy Iteration Agent\"\n",
    "        self.env = env\n",
    "        self.discount = discount\n",
    "        self.theta = theta\n",
    "        self.V = {\"low\": 0, \"high\": 0}\n",
    "        self.pi = {\"low\": self.env.getPossibleActions(\"low\")[0], \"high\": self.env.getPossibleActions(\"high\")[0]}\n",
    "\n",
    "        # train\n",
    "        self.train(iterations)\n",
    "\n",
    "    def train(self, iterations):\n",
    "\n",
    "        states = [\"low\", \"high\"]  # states are numbered\n",
    "        start_time = time.time()\n",
    "        for i in range(iterations):\n",
    "\n",
    "            # Policy Evaluation\n",
    "            while True:\n",
    "                delta = 0\n",
    "                for state in states:\n",
    "                    v = self.V[state]\n",
    "                    value = self.calculate_state_value(state)\n",
    "                    self.V[state] = value\n",
    "                    delta = max(delta, abs(v - self.V[state]))\n",
    "\n",
    "                if delta < self.theta:\n",
    "                    break\n",
    "            \n",
    "            # Policy Improvement\n",
    "            policy_stable = True\n",
    "            for state in states:\n",
    "                old_action = self.pi[state]\n",
    "                actions = self.env.getPossibleActions(state)\n",
    "                action_value_list = []\n",
    "                for action in actions:\n",
    "                    action_value_list.append(self.calculate_q_value(state, action))\n",
    "\n",
    "                self.pi[state] = actions[action_value_list.index(max(action_value_list))]\n",
    "\n",
    "                if old_action != self.pi[state]:\n",
    "                    policy_stable = False\n",
    "            if policy_stable:\n",
    "                print(f\"Stopping criterion satisfied after {i} iterations.\\n\")\n",
    "                print(f\"Value function is: {self.V}\\n\")\n",
    "                print(f\"Optimal policy is: {self.pi}\\n\")\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(f\"Execution time: {elapsed_time:.4f} seconds\\n\")  # Print execution time\n",
    "\n",
    "                break\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    def getValue(self, state):\n",
    "        return self.V[state]\n",
    "\n",
    "    def getPolicy(self, state):\n",
    "        return self.pi[state]\n",
    "\n",
    "    def getAction(self, state):\n",
    "        return self.getPolicy(state)\n",
    "    \n",
    "    def calculate_state_value(self, state):\n",
    "       action = self.pi[state]\n",
    "       sum = self.calculate_q_value(state, action)\n",
    "        \n",
    "       return sum\n",
    "\n",
    "    def calculate_q_value(self,state,action):\n",
    "        sum = 0\n",
    "        for transition in self.env.getTransitionStatesandProbs(state, action):\n",
    "            next_state, reward, prob, _ = transition\n",
    "            sum += prob * (reward + self.discount * self.V[next_state])\n",
    "        return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b628f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping criterion satisfied after 1 iterations.\n",
      "\n",
      "Value function is: {'low': 24.77519542578896, 'high': 27.536435654499513}\n",
      "\n",
      "Optimal policy is: {'low': 'recharge', 'high': 'search'}\n",
      "\n",
      "Execution time: 0.0005 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = RecycleEnv( r_search=4, r_wait=1)\n",
    "agent = PolicyIterationAgent(env=env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a0f2cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping criterion satisfied after 39 iterations.\n",
      "\n",
      "Value function is: {'low': 24.773480007530324, 'high': 27.534806007153808}\n",
      "\n",
      "Optimal policy is: {'low': 'recharge', 'high': 'search'}\n",
      "\n",
      "Execution time: 0.0004 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = RecycleEnv( r_search=4, r_wait=1)\n",
    "agent = ValueIterationAgent(env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c3269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
